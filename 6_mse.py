# -*- coding: utf-8 -*-
"""6_MSE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eVH-FEP2MzbkZ2IcshUccF1S3zV-5ZrU
"""

import numpy as np

"""# MSE 구현하기

- 신경망에서의 손실 함수는 정답레이블이 원핫인코딩 되어있는 경우에 계산한다.

- 안되어있으면 해야한다.
"""

# 모델이 2로 예측했을 확률이 0.6
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]

# 실제 정답은 2
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

# 이 경우의 Loss를 확인한다.(MSE를 이용해서...)

# MSE 함수 구현
def mean_squared_error(y, t):
    return 0.5 * np.sum((y-t) ** 2)

# 정답이 2일 확률이 가장 높다고 측정함 0.6
print("손실함수값 : {:.3f}".format(mean_squared_error(np.array(y), np.array(t))))

# 정답이 7일 경우 (오답일 경우)에 함수 값.
# loss값이 높기 때문에 우리의 목표는 손실값이 위의 손실값처럼 되게 하는것.
y_w = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
print("손실함수값 : {:.3f}".format(mean_squared_error(np.array(y_w), np.array(t))))

"""# 교차 엔트로피 오차"""

def cross_entropy_error(y, t):
    delta = 1e-7 # 소수점 아래로 7자리
    return -np.sum(t * np.log(y + delta))

# 교차 엔트로피를 썼을 때 정답이 2일 확률
print("손실함수값 : {:.3f}".format(cross_entropy_error(np.array(y), np.array(t))))

# 교차 엔트로피를 썼을 때 오답을 준 상황 7로
print("손실함수값 : {:.3f}".format(cross_entropy_error(np.array(y_w), np.array(t))))

"""# 수치미분 구현(나쁜 구현)"""

# 정석이지만 컴퓨터로 쓰기 위해서는 좋은 방법은 아니다.

def numerical_diff(f, x):
    h = 10e-50
    return (f(x+h)-f(x)) / h

# 컴퓨터는 소수점 표현에 한계가 있다. 
print(np.float32(1e-50))

"""# 수치미분 구현(적당한 값)"""

# 대신 오차가 생기게된다. 소수점자리가 적으니
# 이걸 보완하기 위해서 중앙 차분, 중심 차분이라는 것이 있다. 

def numerical_diff(f, x):
    h = 1e-4 # 0.0001

    # 두 점 사이의 중앙값을 사용하기 때문에 2로 나눠집니다.
    return (f(x+h) - f(x-h)) / (2*h)

"""## 수치 미분의 예"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pylab as plt
# %matplotlib inline

def function_1(x):
    return 0.01*x**2 + 0.1*x 

x = np.arange(0.0, 20.0, 0.1) # 0에서 20까지 0.1 간격의 배열을 생성
y = function_1(x)

plt.xlabel('x')
plt.ylabel('f(x)')
plt.plot(x, y)
plt.show()

print('x가 5일 때 미분값 : {}'.format(numerical_diff(function_1, 5)))
print('x가 10일 때 미분값 : {}'.format(numerical_diff(function_1, 10)))

def tangent_line(f, x):
    d = numerical_diff(f, x)
    y = f(x) - d*x
    return lambda t: d*t + y

fig, axes = plt.subplots(1,2, figsize=(20,8))

x = np.arange(0.0, 20.0, 0.1)
y = function_1(x)
axes[0].set_xlabel("x")
axes[0].set_ylabel("f(x)")

tf = tangent_line(function_1, 5)
y2 = tf(x)

axes[0].plot(x, y)
axes[0].plot(x, y2)
axes[0].set_title("x = 5")

axes[1].set_xlabel("x")
axes[1].set_ylabel("f(x)")

tf = tangent_line(function_1, 10)
y2 = tf(x)

axes[1].plot(x, y)
axes[1].plot(x, y2)
axes[1].set_title("x = 10")
plt.show()

"""# 편미분

- 한쪽 방향만 고려하면서 미분한다.
- 나머지 변수는 전부 무시한 채(상수로 생각하고) 로 미분한다.
"""

def function_2(x):
    return x[0] ** 2 + x[1] ** 2

"""- x0 = 3, x1 = 4 일 때 x0에 대한 편미분을 구해보자"""

# x1이 4일 경우에 x0에 대한 편미분 값.(미분을 구하는 식)
def function_tmp1(x0):
    return x0 ** 2 + 4.0 ** 2

print(numerical_diff(function_tmp1, 3.0))

# x0이 3일 경우에 x1에 대한 편미분 값.(미분을 구하는 식)
def function_tmp2(x1):
    return x1 ** 2 + 3.0 ** 2

print(numerical_diff(function_tmp2, 4.0))

# x0=5, x1=3일 때의 편미분 값을 각각 구하시오.
def function_tmp3(x0):
    return 0.8 * x0 ** 2 + 0.1 * 3.0 ** 2 + 0.3

print(numerical_diff(function_tmp3, 5.0))

def function_tmp4(x1):
    return 0.8 * 5.0 ** 2 + 0.1 * x1 ** 2  + 0.3
            
print(numerical_diff(function_tmp4, 3.0))

"""# 기울기에 대한 이야기"""

# 각 변수에 대한 모든 편미분을 벡터로 정리
# 기울기라고 한다.
def numerical_gradient(f, x):
    h = 1e-4

    # 기울기를 저장할 때 배열(편미분 값을 벡터로 저장한 것.) -> 기울기
    grad = np.zeros_like(x) # 들어온 x의 개수만큼 0으로 채워진 배열 생성하기
    
    for idx in range(x.size):
        # 값을 하나씩 꺼내와서
        tmp_val = x[idx]
        # 미분 계산 실행
        # f(x+h) 계산
        x[idx] = tmp_val + h
        fxh1 = f(x)
        
        # f(x-h) 계산
        x[idx] = tmp_val - h
        fxh2 = f(x)
        
        # 실제 미분 실행
        grad[idx] = (fxh1 - fxh2) / 2*h # 미분 수행
        x[idx] = tmp_val # 원래 값으로 복원하기
    
    return grad

