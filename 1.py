# -*- coding: utf-8 -*-
"""1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LAIVEEJHlYW7xnuX5fVQ7ixB9q5ZLObB

# 사용할 데이터셋 다운로드 하기
"""

import os
import tarfile
import urllib.request

# os를 사용하면 path설정을 할 때 오류를 잡아줄 수 있다.

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL  = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url= HOUSING_URL, housing_path= HOUSING_PATH):
  os.makedirs(housing_path, exist_ok= True)
  tgz_path = os.path.join(housing_path, "housing.tgz")
  urllib.request.urlretrieve(housing_url, tgz_path)
  housing_tgz = tarfile.open(tgz_path)
  housing_tgz.extractall(path= housing_path)
  housing_tgz.close()
  
fetch_housing_data()

# 다운 받은 데이터셋( csv ) 파일을 pandas 데이터프레임으로 전환
import pandas as pd
def load_housing_data(housing_path= HOUSING_PATH):
  csv_path = os.path.join(housing_path, "housing.csv")
  return pd.read_csv(csv_path)

housing = load_housing_data()
housing.head()

# median_house_value is label

housing.info()

housing.describe()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
housing.hist(bins=50, figsize=(20, 15))
plt.show()

#  정규분포가 되어있는 데이터를 좋아함.
#  그래야 머신러닝을 돌렸을 때 적합한 데이터를 받아볼 수 있다.
#  비스니스 분석 과정.

#  트레인 테스트를 나누는 이유
#  만약 선생님이 알려준 문제만 풀  줄 아는 학생이라면 새로운 문제를 풀  수가 없다.
#  트레인과 테스트로 나누는 이유는 다양한 문제를 풀  수 있게 하게끔  할 수 있게 해준다..
#  (훈련 검증)으로  먼저  만들고 (테스트)로 예측한다..  실제로는 3단계이다..
#  새로운 데이터에 대해서 예측이 잘 안되는 현상을 스누피 편향이라고 함.
#  훈련 세트로 훈련을 하고, 테스트 세트로 테스트를 진행 하면 알고리즘 입장에서는 처음보는 데이터인 
#  테스트 세트에서 일정 오차가 발생할 것입니다. 테스트 세트에서의 오차를 '일반화 오차'라고 이야기 합니다. 
#  데이터 전처리가 중요하다.

import numpy as np
def  split_train_test(data,  test_ratio=0.2):
  suffled_indices  =  np.random.permutation(len(data))   #  랜덤으로 섞으면서 (permutation인덱스를 섞어줌)리스트로 만들어준다..
  test_set_size  =  int(len(data) * test_ratio)  # 갯수를 세기 위해서 int로 캐스팅  함..
  test_indices = suffled_indices[:test_set_size]  #  20% 비율의  랜덤 인덱스
  train_indices = suffled_indices[test_set_size: ]   #  80%  비율의 랜덤 인덱스
  return data.iloc[train_indices], data.iloc[test_indices] # iloc으로 쓴 이유는 랜덤하게 섞인 '인덱스'이니까.

  #  머신러닝이 보지 말아야할 데이터는 절대 보면 안된다.
  #  위의 예시처럼 언젠가 우리의 데이터는 구분없이 모든 데이터를 보게 될것이다..
  #  1.  난수 발생 초깃값을 지정해 준다. ( np.random.seed(42) 사용 )
  #       데이터가 추가되면 다시 섞일 가능성이 있다.  데이터가 많은 경우 불리함..
  #  2.  각 행( 샘플 ) 마다의 해시값을 구하여 20% 보다 작거나 같은 샘플만 테스트 세트로 보낸다.
  #       랜덤이 보장이 된다.  안정적.

from zlib import crc32 # crc32 : 고유값 구할 용도(id  생성기).  원래는 지문값을 확인하려고 쓴다.

def test_set_check(identifier, test_ratio):
  return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2 **32

def split_train_test_by_id(data, test_ratio, id_column):
  ids = data[id_column]
  in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))
  return data.loc[~in_test_set], data.loc[in_test_set]

# 새로운 데이터의 추가는 반드시 행의 끝에서만 일어날 것
# 어떠한 행도 삭제 되지 말 것

df_train, df_test = split_train_test(housing, 0.2)
len(df_train), len(df_test)

housing_with_id = housing.reset_index()

housing_with_id["id"] = housing["longitude"] * 1000 + housing["latitude"]
train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, "id")

"""# 계층적 샘플링"""

housing["income_cat"] = pd.cut(housing["median_income"],
                               bins=[0.,1.5,3.0,4.5,6.,np.inf],
                               labels=[1, 2, 3, 4, 5])
housing[["median_income", "income_cat"]].head()

"""# 시각화"""

housing["income_cat"].hist()

#  사이킷런에는 계층을 활용한 샘플링을 지원하는 StratifiedShuffleSplit이 있다.
from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

for train_index, test_index in split.split(housing, housing["income_cat"]):
  strat_train_set = housing.loc[train_index]
  strat_test_set = housing.loc[test_index]

strat_test_set["income_cat"].value_counts() / len(strat_test_set)

"""#  주택가격 데이터 분석을 위한 탐색과 시각화"""

# 필요하다면 계층을 만들고 나머지는 사이키런에 맡기면 알아서 나누어 준다.

housing = strat_train_set.copy()
housing.head()

housing.plot(kind='scatter', x='longitude', y='latitude')

housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)

housing.plot(kind='scatter', x='longitude', y='latitude', alpha=.4, 
                     s=housing["population"] / 100, label="population", figsize=(10, 7), 
                    c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True)
plt.legend()

# Download the California image
images_path = os.path.join(".", "images", "end_to_end_project")
os.makedirs(images_path, exist_ok=True)
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
filename = "california.png"
print("Downloading", filename)
url = DOWNLOAD_ROOT + "images/end_to_end_project/" + filename
urllib.request.urlretrieve(url, os.path.join(images_path, filename))

import matplotlib.image as mpimg
california_img=mpimg.imread(os.path.join(images_path, filename))
ax = housing.plot(kind="scatter", x="longitude", y="latitude", figsize=(10,7),
                       s=housing['population']/100, label="Population",
                       c="median_house_value", cmap=plt.get_cmap("jet"),
                       colorbar=True, alpha=0.4,
                  
                      )

# extent  가지고 있는 데이터 위도 경도의 범위
plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5, 
           cmap=plt.get_cmap("jet"))
plt.ylabel("Latitude", fontsize=14)
plt.xlabel("Longitude", fontsize=14)

plt.legend(fontsize=16)
plt.show()

"""#  상관관계 조사하기"""

#  머신이 숫자만을 사용하기 때문에...
#  중요하고 덜 중요하고의 차이는 데이터를 가공하면서 새로운 상관관계를 찾아내거나 할 수  있다.
#  양의 상관관계,,  음의 상관관계에 절대값을 씌운것이 피어슨 상관계수.
#  0  <  0.3  <  0.7  <  1  
#  1  이상 나올 수 없음.

#  corration
corr_matrix = housing.corr()

# 중간 주택 가격에 대한 다른 특성들과의 상관계수 구하기
# 기준 컬럼으로 상관관계 계산.
corr_matrix['median_house_value'].sort_values(ascending=False)

from pandas.plotting import scatter_matrix

features = ["median_house_value", "median_income", "total_rooms", "housing_median_age"]

scatter_matrix(housing[features], figsize=(13, 9))
plt.show()

housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.3, figsize=(12, 8))

"""# 특성 조합 보기"""

#  *  연관관계가 있다고 생각이 될  때,  연관성을 보고 싶을 때
#  /  비율만 구하고 싶을 때 
#  +  - 는 사용하지 않음.  
#  상관관계가 너무 (부족))없다고 생각할 때 특성을 '인간''이 생각해 보는 것이 '특성조합''

housing["rooms_per_household"] = housing["total_rooms"] / housing["households"]
housing["bedrooms_per_room"]   = housing["total_bedrooms"] / housing["total_rooms"]
housing["population_per_household"] = housing["population"] / housing["households"]

corr_matrix = housing.corr()
corr_matrix["median_house_value"].sort_values(ascending=False)

"""#  머신러닝을 위한 데이터 준비"""

# feature와 label  나누기
#  feature  ->  특성,  문제
#  label  ->  정답,  y값,  타겟

housing = strat_train_set.drop("median_house_value", axis=1)  #  훈련 데이터 셋(feature)준비 
housing_labels = strat_train_set["median_house_value"].copy()  #  훈련 레이블(target, label)  준비

"""###  차원 정리
10  (점 -  스칼라)  0차원  <br><br>
[0,  3]  (선 -  벡터)  1차원 <br><br>
[[1,  2],<br>
 [3,  4]]  (면 -  매트릭스))  2차원<br><br>
 입체는 텐서 3차원 ~
"""

housing.info()

"""#  좋은 데이터를 알고리즘이 알 수 있게 해주는 데이터 정제

NaN값을 제거


1.  해당 구역을 제거 
2.  전체 특성 삭제 
3.  어떤 값으로 채우기 ( 0 또는 평균, 중간값  
등으로... )
4.  사이킷런의 SimpleImputer는 이러한 누락값들을 손쉽게 채울 수 있도록 해줍니다. 중간값으로 채운다고 가정해 보겠습니다.
"""

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="median")

housing_num = housing.drop("ocean_proximity", axis=1)

# fit -> 적용
# imputer  ->  변환기
imputer.fit(housing_num)

#  각 컬럼에 적용되는 median값.
imputer.statistics_

housing_num.median().values

#  변환 -  transform
# 결측치가 채워진 numpy  배열 반환
X = imputer.transform(housing_num)
X

housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)
housing_tr.head()

housing_tr.info()

"""#  텍스트와 범주형 특성 다루기

만약 타이틀이나 노트 같은 문자열일  때  토크나이저를 쓰면 편하게  정제할 수 있다..
그냥은 원-핫 인코딩이 안되기 때문
"""

housing_cat = housing[["ocean_proximity"]]
housing_cat.head(10)

from sklearn.preprocessing import OrdinalEncoder
ordinal_encoder = OrdinalEncoder()

housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat.values)
housing_cat_encoded[:10]

# 카테고리의 성격을 띌 때는 데이터를 사용하려면 원핫인코딩을 해야한다.
ordinal_encoder.categories_

#  one-hot  인코딩이 되어야함.
from sklearn.preprocessing import OneHotEncoder
cat_encoder = OneHotEncoder()
#  fit_transform  핏하고 트랜스폼을 동시에 함.  변환(transform) 및 적용(fit)
housing_cat_one_hot = cat_encoder.fit_transform(housing_cat)
housing_cat_one_hot
# 바로 만들면 메모리에 부담을 주기 때문에 희소 영역에 일단 저장 된다.
# housing_cat_one_hot.toarray() 로 확인할 수 있음.

"""# 변환기"""

# from sklearn.base import BaseEstimator, TransformerMixin
# #  TransformerMixin -> 자동으로 상속되는 클래스에 fit_transform()  메소드를 생성해준다..

# rooms_ix, bedroom_ix, population_ix,  households_ix = 3, 4, 5, 6

# class CombinedAttributesAdder(BaseEstimator, TransformerMixin):

#   # 생성자,  변환기의 정책을 담당(  초기화  ),  할지 말지에 대한 스위치?,  실질적인 작업 하지 않음.
#   def __init__(self,  add_bedrooms_per_room = True):
#     self.add_bedrooms_per_room = add_bedrooms_per_room


#   # 자동으로 수퍼로 올라감 X가,  알고리즘에 들어감.  여기에서는 그대로 메모리에 집어넣을 용도로 사용.
#   #  적용  시키는 부분 
#   #  데이터를 적용시키는 구간.
#   #  메소드 체이닝 
#   #  사이키런의 변환기, 추정기 클래스로 인정받기 위해서는 fit이 무조건 잇어야한다..
#   def fit(self, X):
#     return self


  # def transform(self, X):
  #  # rooms_per_household 변환
  #  # loc(행,  열) 의 구조  넘파이기  때문에 숫자만 가능하다,  
  #  rooms_per_household = X[:, rooms_ix] / X[:, households_ix]

  #  #  population_per_household 변환 
  #  population_per_household = X[:, population_ix] / X[:, household_ix]

  #  # 초기화 했었던 정책에 의한 변환
  #  #  np.c_  열방향으로 붙혀준다.  
  #    if self.add_bedrooms_per_room:
  #      bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
  #      return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]

  #   else:
  #     return np.c_[X, rooms_per_household, population_per_household]

  # def transform(self, X):
  #   rooms_per_household = X[:, rooms_ix]  / X[:, households_ix] # 가구당 방 비율 구하기
  #   population_per_household = X[:, population_ix] / X[:, households_ix] # 가구당 인구수 구하기

  #   if self.add_bedrooms_per_room:
  #     bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
  #     return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]

  #   else:
  #     return np.c_[X, rooms_per_household, population_per_household]

from sklearn.base import BaseEstimator, TransformerMixin

#  컬럼 번호
rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6

class CombinedAttributesAdder(BaseEstimator, TransformerMixin):

  def __init__(self, add_bedrooms_per_room = True):
    self.add_bedrooms_per_room = add_bedrooms_per_room

  def fit(self, X):
    return self

  def transform(self, X):
    rooms_per_household = X[:, rooms_ix]  / X[:, households_ix] # 가구당 방 비율 구하기
    population_per_household = X[:, population_ix] / X[:, households_ix] # 가구당 인구수 구하기

    if self.add_bedrooms_per_room:
      bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
      return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]

    else:
      return np.c_[X, rooms_per_household, population_per_household]

attr_adder = CombinedAttributesAdder(add_bedrooms_per_room= False)
housing_extra_attribs = attr_adder.transform(housing.values)
housing_extra_attribs[:3]

"""# 특성 스케일링

머신러닝에 집어넣기 전에 스케일링 작업을 무조건 해야한다.  안하면 했을  때와의성능차이가 너무 많이 남.

#  2.11.5 변환 파이프라인 만들기
"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

# 결측치 채우고
#  fit transform하여 특성값 추가
#  스케일러 선택
# 마지막 단계는 결과물을 내는 과정,  변환기 추정기 모두 사용 가능.
#  이전의 단계는 변환 단계이다.  
#  단계로 함.


#  파이프 라인은 알고리즘을 만들기 위한 과정들을 엮어 놓은 것
#  제일 마지막 과정은 변환기 /  추정기 상관없음
#  마지막 이전 과정은 무조건 변환기여야만 한다.
#  그말인 즉슨 fit_transform 또는 fit, transform  메소드가 있어야함.

#  모델을 파일화 가능하다.  
num_pipeline = Pipeline([
                         ('imputer', SimpleImputer(strategy="median")),
                         ('attribs_attr', CombinedAttributesAdder()),
                         ('std_scaler', StandardScaler())
])
housing_num_tr = num_pipeline.fit_transform(housing_num)

"""#  특정 컬럼만 선택해서 변환하는 ColumnTransformer"""

from sklearn.compose import ColumnTransformer

num_attribs = list(housing_num) # 수치형 데이터 가지고 오기
cat_attribs = ["ocean_proximity"]

full_pipeline = ColumnTransformer([
                                   ("num", num_pipeline, num_attribs), # 수치 데이터는 num_pipeline으로 처리
                                   ("cat", OneHotEncoder(), cat_attribs)  #  카테고리 데이터는 OneHotEncoder로 따로 처리
])

housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared[:3]

"""# 2.12 머신러닝 알고리즘 모델 선택과 훈련"""

from sklearn.linear_model import LinearRegression

# 선형 회귀 모델로 훈련 시킴.

lin_reg = LinearRegression()
lin_reg.fit(housing_prepared, housing_labels)

example_train_data  =  housing.iloc[:5]
example_labels  =  housing_labels.iloc[:5]

example_data_prepared  =  full_pipeline.transform(example_train_data)

print("예측  :  {}".format(lin_reg.predict(example_data_prepared)))

print(example_labels)

"""# 2.12.2 모델의 성능 평가하기"""

from sklearn.metrics import mean_squared_error
housing_predictions = lin_reg.predict(housing_prepared)

#MSE를 먼저 구하자  -  평균제곱오차
lin_mse = mean_squared_error(housing_labels, housing_predictions)

#  Root를 씌우면 RMSE  완성
lin_rmse = np.sqrt(lin_mse)
lin_rmse

# 그렇지만 문제가 오차가 너무 많이 난다는 것이다..  훈련데이터로 했음에도 불구하고..
# 모델이 과소적합  되어있다는 것을 알 수 있어야 한다..
#  과소 과대일 경우에 데이터를 수정하거나(재처리)  모델을 바꾸면 된다..

# 모델 바꾸기
#  초보자들이 쓰기에 좋은 모델
#  이후에 나온 것이 랜덤포레스트 모델
from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor()
tree_reg.fit(housing_prepared, housing_labels)

housing_predictions = tree_reg.predict(housing_prepared)
tree_mse = mean_squared_error(housing_labels, housing_predictions)
tree_rmse = np.sqrt(tree_mse)
tree_rmse

#  0.0이 나왓는데 사실 불안정하다..
#  과대적합 되었다는 가능성이 높다 선형회귀의 결과와 비교했을 때..

"""# 교차  검증

테스트 세트는 론칭 전까지 아껴놔야한다.  그래서 훈련 데이터를 검증과 훈련으로 나누는 작업을 하여 다시금 모델링을 해본다.

##  검증  셋트  1
##  훈련  셋트  3
이런식으로 나누어 검증 셋트를 바꿔가며 총 4번의 교차 검증을 한다..(케이스마다 횟수는 다름)

검증 셋트는 validation 훈련 셋트는 fold라고 부른다.

사이키런의 cross_val_score은 얼마나 '가깝게'  맞췄느냐.  점수가 높을 수록 높으면 좋다(정확도  Accuracy)  오차율은 낮아야 좋고 cross_val_score는 값이 높아야 좋다.

CV는 폴드의 개수
"""

from sklearn.model_selection import cross_val_score
scores = cross_val_score(tree_reg, housing_prepared, housing_labels,
                         scoring="neg_mean_squared_error", cv=10)


tree_rmse_scores = np.sqrt(-scores)

def display_score(scores):
  print("점수 :  {}".format(scores))
  print("평균 :  {}".format(scores.mean()))
  print("표준편차 :  {}".format(scores.std()))

display_score(tree_rmse_scores)

lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,
                             scoring="neg_mean_squared_error", cv=10)

lin_rmse_scores = np.sqrt(-lin_scores)
display_score(lin_rmse_scores)

"""# 그럼 이제 랜덤 포레스트로 해보자"""

from sklearn.ensemble import RandomForestRegressor
forest_reg = RandomForestRegressor()
forest_reg.fit(housing_prepared, housing_labels)

# 교차 검증을 하지 않았을 때의 점수
forest_predicted = forest_reg.predict(housing_prepared)
forest_mse = mean_squared_error(housing_labels, forest_predicted)
forest_rmse = np.sqrt(forest_mse)
forest_rmse

# 교차 검증을 수행했을 때의 점수
forest_mse_scores = cross_val_score(RandomForestRegressor(), housing_prepared, housing_labels, 
                                     scoring="neg_mean_squared_error", cv=8)

forest_rmse_scores = np.sqrt(-forest_mse_scores)

display_score(forest_rmse_scores)

# import joblib 모델 저장

# joblib.dump(forest_reg, "my_model_50212.28868.pkl")

"""#  그리드 탐색"""

from sklearn.model_selection import GridSearchCV

#  하이퍼 파라미터 딕셔너리
param_grid = [
              {"n_estimators": [3, 10, 30], "max_features":[2, 4, 6, 8]},
              {"bootstrap": [False], "n_estimators": [3, 10], "max_features": [2, 3, 4]},
]

#  사용할 모델
forest_reg = RandomForestRegressor()

#  하이퍼 파라미터를 찾기위한 GridSearch + Cross Validation  설정
grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                           scoring='neg_mean_squared_error',
                           return_train_score=True)

#  훈련
grid_search.fit(housing_prepared, housing_labels)

grid_search.best_params_  # 최고의 성능을 낸 하이퍼 파라미터

grid_search.best_estimator_  #  최고의 성능을 낸 추정기

res = grid_search.cv_results_
for mean_score, params in zip(res["mean_test_score"], res["params"]):
  print(np.sqrt(-mean_score), params)

final_model = grid_search.best_estimator_  #  최고의 성능을 낸 모델을 가져오기

#  테스트 세트를 준비한다..
X_test = strat_test_set.drop("median_house_value", axis=1)
y_test = strat_test_set["median_house_value"].copy()

#  훈련 세트와 똑같이 변환기에 넣고 돌린다..
X_test_prepared = full_pipeline.transform(X_test)

#  예측과 오차를 확인하기
final_predictions = final_model.predict(X_test_prepared)

#  
final_mse = mean_squared_error(y_test, final_predictions)
final_rmse = np.sqrt(final_mse)

print(final_rmse)

from scipy import stats
confidence = 0.95 # 신뢰 구간 만들기
squared_errors = (final_predictions - y_test) ** 2
np.sqrt(stats.t.interval(confidence, len(squared_errors)-1,
                         loc=squared_errors.mean(),
                         scale=stats.sem(squared_errors)))

