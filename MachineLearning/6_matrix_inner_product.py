# -*- coding: utf-8 -*-
"""6_matrix_Inner Product.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jUIdIOGCjzE2ZxcjQG-YvbfkctV8ahVa
"""

import numpy as np

"""# 다차원 배열 계산하기"""

# 배열은 대문자로 표기하는 것이 관례
A = np.arange(1, 5).reshape(2, 2)
B = np.arange(5, 9).reshape(2, 2)

A.shape, B.shape

"""numpy의 dot함수를 활용하면 행령의 내적을 구할 수 있다.

행렬의 내적은 교환법칙이 성립하지 않는다.
"""

np.dot(A, B)

np.dot(B, A)

"""## shape이 서로 다를 때의 행렬의 내적 규칙"""

#  앞에서 곱해지는 뒤 숫자와 뒤에서 곱해지는 앞의 숫자가 같으면  shape이 달라도 가능하다.

C = np.arange(1, 7).reshape(2, 3)
D = np.arange(1, 7).reshape(3, 2)
C

D

C.shape, D.shape

C, D

np.dot(C,  D)

np.dot(D,  C)

E = np.arange(1, 5).reshape(2, 2)
E.shape

np.dot(E, C)

"""## 차원수가 다를 경우의 행렬 내적 구하기"""

F = np.arange(1, 7).reshape(3, 2)
G = np.array([7, 8])
F.shape, G.shape

F.ndim, G.ndim

np.dot(F, G)

"""#  신경망에서의 행렬 내적"""

X = np.array([1, 2])
W = np.array([[1, 3, 5],
                      [2, 4, 6]])
Y = np.dot(X, W)

Y

X = np.array([1.0, 0.5])

# 1층의 가중치들
W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])

# 1층의 편향들
B1 = np.array([0.1, 0.2, 0.3])

X.shape, W1.shape, B1.shape

A1 = np.dot(X, W1) + B1
A1

"""# 활성화 함수 적용하기

-  시그모이드
"""

result = 1 /  (1 + np.exp(-A1))
result

# 시그모이드 함수 만들기
def sigmoid(x):
  return 1 / ( 1 + np.exp(-x) )

Z1 = sigmoid(A1)
Z1

"""## 2층 꾸며주기"""

W2 = np.array([[0.1, 0.4],
              [0.2, 0.5],
              [0.3, 0.6]])

B2 = np.array([0.1, 0.2])

W2.shape, B2.shape

A2 = np.dot(Z1, W2) + B2
Z2 = sigmoid(A2)
Z2

"""##  3층을 출력층으로 생각한다면?

-  항등함수
  -  패턴을 맞추기 위해 사용할 뿐.
-  cross entropy
- softmax  등을 생각할 수 있다.
"""

# 항등함수 만들기
def identity_function(x):
  return x

#  세번째 층은 출력층이다.
W3 = np.array([[0.1, 0.3], 
               [0.2, 0.4]])

B3 = np.array([0.1, 0.2])

# 출력층도 기존 은닉층과 똑같이 입력값에 대한 가중치 내적과 평향 덧셈이 이루어진다.
A3 = np.dot(Z2, W3) + B3
Y = identity_function(A3)
Y

"""##  출력층 설계

-  소프트맥스 구현
"""

# 입력 값 
a = np.array([0.3, 2.9, 4.0])

# 모든 입력 신호에 대한 지수 함수 적용(분자 값)
exp_a = np.exp(a)
exp_a

# 분모 만들기

sum_exp_a = np.sum(exp_a)
sum_exp_a

y = exp_a / sum_exp_a
y

# 함수 만들기
def softmax(a):
  exp_a = np.exp(a)
  sum_exp_a = np.sum(exp_a)
  y = exp_a / sum_exp_a
  return y

a = np.array([1010, 1000, 990])
softmax(a)

"""##  향상된 softmax"""

# 상수값 C는 일반적으로 배열에서 가장 큰 값을 사용한다..
c = np.max(a)
c

print(a-c)

np.exp(a-c) / np.sum(np.exp(a-c))

# 향상 소프트 맥스 재구현
def softmax_up(a):
  c = np.max(a) # 최대값 구하기
  exp_a = np.exp(a - c) # logC를 더한 것과 같은 효과

  sum_exp_a = np.sum(exp_a)
  y = exp_a / sum_exp_a

  return y

a = np.array([0.3, 2.9, 4.0])
np.sum(softmax_up(a))

softmax_up(a)

"""## MNIST 손글씨 데이터 셋 사용하기

-  데이터가 있지만 코랩이라 케라스 사용
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import matplotlib.pyplot as plt

# %matplotlib inline

from tensorflow.keras import datasets
mnist = datasets.mnist

# 튜플 형태로 묶어주면 트레인과 테스트를 구분해준다.
(X_train, y_train), (X_test, y_test) = mnist.load_data()

X_train.shape, y_train.shape

X_test.shape, y_test.shape

image = X_train[0]
image.shape

plt.imshow(image, 'gray')

image2 = X_train[1]
plt.imshow(image2, 'gray')
plt.show

image.shape

i2 = image2.flatten()

i2.shape

"""##  MNIST를 위한 신경망 구축하기"""

#  활성화 함수 구현 ->  시그모이드
def sigmoid(x):
  return 1 / (1 + np.exp(-x))

def get_test_data():
    (X_train, y_train), (X_test, y_test) = mnist.load_data()
    image_size = X_test.shape[0]
    X_test_reshaped = X_test.reshape(image_size, -1)

    return X_test_reshaped, y_test


# 훈련된 신경망 가져오기
def init_network():
  import pickle
  with open("./sample_weight.pkl", 'rb') as f:
    network = pickle.load(f)

  return network

# 예측함수
def predict(network , x):
  W1, W2, W3 = network['W1'], network['W2'], network['W3']
  b1, b2, b3 = network['b1'], network['b2'], network['b3']

  ## 구현 시작
  # 흘러들어온 데이터와 현재 층의 가중치의 내적을 구하고 편향을 더한다.
  #  활성화 함수를 입혀준다.

  # Layer 1 계산
  z1 = np.dot(x, W1) + b1
  a1 = sigmoid(z1)

  # Layer 2 계산
  z2 = np.dot(a1, W2) + b2
  a2 = sigmoid(z2)

  # Layer 3 (출력층 계산)  활성화 함수가 softmax
  z3 = np.dot(a2, W3) + b3
  y = softmax_up(z3)

  return y

X, y = get_test_data()
X.shape, y.shape

network = init_network()

accuracy_cnt = 0 # 맞춘 개수를 저장(맞추면 1  증가)

for i in range(len(X)):
  pred = predict(network, X[i]) # 사진 한장 당의 softmax결과가 나옴.
  pred = np.argmax(pred) # 확률이 가장 높은 원소의 인덱스 얻어오기

  if pred == y[i]:
    accuracy_cnt += 1

print(float(accuracy_cnt) / len(X))

"""##  배치처리

- 뭉텅이로 검사를 할 수 있게 하는 기능
- 대량의 데이터를 쪼개서 관리하는 배치  처리
- 하이퍼 파라미터가 될  것이다.
"""

x, _ = get_test_data()
network = init_network()

W1, W2, W3 = network['W1'], network['W2'], network['W3']

print("입력 데이터의 shape : {}".format(x.shape))
print("첫 번째 데이터의 shape : {}".format(x[0].shape))
print("가중치 W1의 shape : {}".format(W1.shape))
print("가중치 W2의 shape : {}".format(W2.shape))
print("가중치 W3의 shape : {}".format(W3.shape))

X, y = get_test_data()

network = init_network()


# batch  크기 지정
# batch란?
# 예를 들어 10000개의 데이터를 100개의 묶음(batch_size)으로 만들면 100개의 배치가 생긴다.

batch_size = 100 # 배치 크기 지정
accuracy_cnt = 0

for i in range(0, len(X), batch_size):
    x_batch = X[i : i+batch_size] # 순서대로 100개씩 데이터를 쪼갬
    y_batch = predict(network, x_batch)  #  데이터를 100개씩 예측
    
    p = np.argmax(y_batch, axis = 1)  #  100개의 예측 데아터에서 가장 높은 값의 인덱스를 추출
    # axis가 0이면 열로 처리하기 때문에 1을 줘야한다.
    accuracy_cnt += np.sum( p == y[i : i+batch_size])  #  100개씩 정답의 합을 구함

print("정확도 : {}".format(float(accuracy_cnt) / len(X)))

