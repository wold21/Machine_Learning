# -*- coding: utf-8 -*-
"""4_bining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UkBHj95aK0AcYm9hNtNmrEV5OYqOXKAP
"""

from sklearn.model_selection import train_test_split
import mglearn
import numpy as np
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt



X, y = mglearn.datasets.make_wave(n_samples=100)

# 최소값,  최대값,  개수
bins = np.linspace(-3, 3, 11)
print("구간: {}".format(bins))

#  나눈 구간에 데이터를 넣는다.  np.digitize
#   구간 어디에 데이터가 있는지?
which_bin = np.digitize(X, bins=bins)
print('\n데이터 포인트:\n', X[:5])
print('\n데이터 포인트의 소속구간:\n', which_bin[:5])

# 카테고리 데이터를 원핫 인코딩

from sklearn.preprocessing import OneHotEncoder
#OneHotEncoder를 사용
encoder = OneHotEncoder(sparse=False) # 희소행렬 형태로 나타내지 않음(각 아이템의 위치값을 나타내지 않음)

#encoder.fit은 which_bin에 나타낸 유일한 값을 찾습니다.
encoder.fit(which_bin)

#One-hot-encoding으로 변환
X_binned = encoder.transform(which_bin)
print(X_binned[:5])

#  shape  확인
print("X_binned.shape: {}".format(X_binned.shape))

print("X_shape : {}".format(X.shape))

"""# 상호 작용과 다항식"""

#  hstack  판다스의 merge랑 비슷함.  옆으로 합칠 수 있게 해줌.
X_combined = np.hstack([X, X_binned]) # hstack을 이용해 구간별 One-hot-encoding된 데이터 추가
print(X_combined.shape)

#  전체 구간별 기울기 평균.
#  맞지 않게 때문에 구간별로 평균을 봐야함.

#  구간별 X를 가지는 과정.
X_product = np.hstack([X_binned, X * X_binned]) # 인코딩된 구간데이터와, 구간과 원본 특성의 곱을 구한 데이터를 추가적으로 합침
print(X_product.shape)

#  특성의 갯수가 너무 적고 선형회귀를 사용해야할  때  
#  이렇게 특성을 추가,  사용해 만들 수 있다..

"""#  원본 특성의 다항식 추가하기

위처럼 구간을 나누지 않고,  원본 특성의 다항식을 추가하는 방법이 있다.

선형회귀를 사용하지만 곡선이 나올 수 있다.

y = WX^2 + WX + h
"""

# 제곱항을 만들 때 PolynomialFeatures사용하면 편리하다.
from sklearn.preprocessing import PolynomialFeatures
# x ** 10 까지의 고차항을 추가합니다.
# 기본값인 "include_bias=True"는 절편을 위해 값이 1인 특성을 추가합니다.
# degree  ->  x에 대한 차수 설정 인자
poly = PolynomialFeatures(degree=10, include_bias=False)
poly.fit(X)

X_poly = poly.transform(X)

#  원본 데이터에 비해 특성이 추가됐다.
print('X_poly.shape: {}'.format(X_poly.shape))

print('X 원소:\n{}'.format(X[:5]))
print()
print('X_poly 원소:\n{}'.format(X_poly[:5]))

print('항 이름:\n{}'.format(poly.get_feature_names()))

reg = LinearRegression().fit(X_poly, y)

line_poly = poly.transform(line)

plt.plot(line, reg.predict(line_poly), label='다항 선형 회귀')
plt.plot(X[:, 0], y, 'o', c='k')
plt.ylabel('회귀 출력')
plt.xlabel('입력 특성')
plt.legend(loc='best')
plt.show()

from sklearn.svm import SVR

for gamma in [1, 10]:
    svr = SVR(gamma=gamma).fit(X, y)
    plt.plot(line, svr.predict(line), label='SVR gamma={}'.format(gamma))

plt.plot(X[:, 0], y, 'o', c='k')
plt.ylabel('회귀 출력')
plt.xlabel('입력 특성')
plt.legend(loc='best')
plt.show()

