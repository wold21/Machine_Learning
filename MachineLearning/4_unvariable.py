# -*- coding: utf-8 -*-
"""4_unvariable.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13h7FMVJMdhu3LHl-a5W9F77ZLa8w4Lou
"""

from sklearn.datasets import load_breast_cancer
from sklearn.feature_selection import SelectPercentile
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt


cancer = load_breast_cancer()

#난수 발생
rng = np.random.RandomState(42)
noise = rng.normal(size=(len((cancer.data)), 50))

# 위에서 발생시킨 랜덤 데이터를 노이즈 특성으로 사용, 처음 30개는 원본 특성, 다음 50개는 노이즈
X_w_noise = np.hstack([cancer.data, noise])

X_train, X_test, y_train, y_test = train_test_split(X_w_noise, cancer.target, random_state=0, test_size=.5)
# f_classif와 SelectPercentile을 이용하여 특성의 50%를 선택하기
select = SelectPercentile(percentile=50)
select.fit(X_train, y_train)

#  훈련세트에 적용하기
X_train_selected = select.transform(X_train)

print("X_train.shape: {}".format(X_train.shape))
print("X_train_selected.shape: {}".format(X_train_selected.shape))

from sklearn.linear_model import LogisticRegression

# 테스트 데이터 변환
X_test_selected = select.transform(X_test)

lr = LogisticRegression()
lr.fit(X_train, y_train)
print("전체 특성을 사용한 점수: {:.3f}".format(lr.score(X_test, y_test)))

lr.fit(X_train_selected, y_train)
print("선택된 일부 특성을 사용한 점수 : {:.3f}".format(lr.score(X_test_selected, y_test)))

# 이 경우에서는 원본 특성이 몇개 없더라도 
# 노이즈 특성을 제거한 쪽의 성능이 더 좋다는 것을 확인할 수 있습니다. 
# 이 예는 인위적으로 간단하게 만든 예제이고, 엇갈리는 경우도 많습니다.
# 하지만 너무 많은 특성 때문에 모델을 만들기가 현실적으로 어려울 때 
# 일변량 분석을 사용하여 특성을 선택하면 큰 도움이 될 수도 있습니다. 
# 또는 많은 특성들이 확실히 도움이 안된다고 생각될 때 사용하면 좋습니다.

"""# 모델 기반 특성 선택

모델이 알아서 특성을 골라준다.

대신 각 특성의 중요도의 순서를 매길 수 있어야함.
"""

from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier

#  threshold  임계값,  중요도  결정?
#  median이 들어갔다는 이야기는 feature_importance  값이 중간은 넘어야 한다는 뜻이다.  

select = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), 
                         threshold='median')

select.fit(X_train, y_train)
X_train_l1 = select.transform(X_train)

print("X_train.shape : {}".format(X_train.shape))
print("X_train_l1.shape : {}".format(X_train_l1.shape))

mask = select.get_support()
# True는 검은색, False는 흰색으로 마스킹
plt.matshow(mask.reshape(1, -1), cmap='gray_r')
plt.xlabel("특성 번호")
plt.yticks([0])
plt.show()

X_test_l1 = select.transform(X_test)
score = LogisticRegression().fit(X_train_l1, y_train).score(X_test_l1, y_test)
print("테스트 점수: {:.3f}".format(score))

"""#  반복적 특성 선택

효과가 아주 좋지만 시간이 오래 걸린다.

특성을 모두 보고 필요없는 것을 버린다.

특성을 하나도 선택하지 않은 상태로 어떠한 종료 조건에 도달할 때까지 특성을 하나씩 추가 하는 방법

모든 특성을 가지고 시작하여 어떤 종료 조건이 될 때까지 특성을 하나씩 제거해가는 방법
"""

