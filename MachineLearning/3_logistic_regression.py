# -*- coding: utf-8 -*-
"""3_Logistic Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PCE1mmYuQl2kv_sQqgy5oXruPMzXjzFN
"""

# 선형 이진 분류
# Logistic Regression (Regression이 붙어있지만 분류기)
# 선형 회귀에  sigmoid함수를 씌운것.
# 선형 회귀에 확률이 추가됨.
# 선형회귀는 직선을 긋기위해 기울기와 절편을 구하는 알고리즘, 그 선과의 오차율.
# 선형 이진 분류는 거기에 확률이 추가되어 선을 어떻게 그려야 하는지 알아낼 알고리즘.
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
cancer = load_breast_cancer()

X_train, X_test, y_train, y_test = train_test_split( cancer.data, 
                                                    cancer.target, 
                                                    stratify=cancer.target, 
                                                    random_state=42)
logreg = LogisticRegression().fit(X_train, y_train)

#  가중치를 조절할 수 있는 c값
#  모델의 규제의 강도를 줄이는 매개변수
#  즉 복잡도를 제어한다는 것.
#  그리하여 복잡도가 높아지면 과대적합이 되고
#  그 값이 너무 낮으면 과소 적합이 될 수 있다.
#  c값이 높아지면 규제가 감소  (  가중치 증가 )
#  c값이 낮아지면 규제가 증가  (  가중치 감소 )
#  회귀의 alpha값과 반대이다.  헷갈리면 안됨.

print("훈련 세트 :  {:.3f}".format(logreg.score(X_train, y_train)))
print("테스트 세트 :  {:.3f}".format(logreg.score(X_test, y_test)))

# 값이 너무 똑같다 과소적합을 의심해봐야함.

# C값을 늘려봤다.
# 성능이 더 좋게 나왔다.

logreg100 = LogisticRegression(C=100).fit(X_train, y_train)
print("훈련 세트 :  {:.3f}".format(logreg100.score(X_train, y_train)))
print("테스트 세트 :  {:.3f}".format(logreg100.score(X_test, y_test)))

