# -*- coding: utf-8 -*-
"""11_2_Tokenization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cGLW4BAV1LysGI9C0rT0_pPD8GEQFbZ7

# Tokenization
- 어디까지가 문장이고 단어인지 컴퓨터에게 알려주는 것.
- 문장을 .으로 구분하거나 단어를 띄어쓰기로 구분했을 시에 문제점
"""

song_text = "I was just guessing at numbers and figures. Pulling your puzzles apart. Questions of science science and progress. Do not speak as loud as my heart."

tokenized_sentence = song_text.split(". ")
tokenized_sentence

"""한글 / 영어 둘다 .으로 구분되어 있는 문장이면 무리없이 토크나이징이 가능하다."""

tokenized_word = song_text.split()
tokenized_word

"""## 띄어쓰기로 영어 문장 내 단어 구분?
- We're Genius!!
- We are Genius!!
- We are Genius

- 형태소 분석기

## 특수문자 제거를 이용해 단어 구분?
- $12.57
- Mr.So
- 192.168.0.1

# 형태소 분석기를 사용하자
- 위와 같은 문제를 벗어나기위해

- TreebankWordTokenizer - Penn Treebank Tokenization 규칙
    - 하이픈으로 구선된 단어는 하나로 유지
    - don't와 같이 어퍼스트로퍼로 접어가 함께하는 단어는 분리해준다.

예시
- I don't care! -> I, do, n't, care, !
- I'm Iron-man -> I, 'm, Iron-man
"""

import nltk
nltk.download('punkt')

sentence = "When you're on a golden-sea. You don't need no memory. Just a place to call your own. As we drift into the zone"

"""# 기본 토크나이저(영어)"""

from nltk.tokenize import word_tokenize
print(word_tokenize(sentence))

"""# WordPunctTokenizer
- 어퍼스트로피 접어를 따로 분리하지 않는 토크나이저
"""

from nltk.tokenize import WordPunctTokenizer
print(WordPunctTokenizer().tokenize(sentence))

"""# TreebankWordTokenizer
- 하이픈으로 연결된 단어는 하나로 유지
- 어퍼스트로피로 연결된 단어는 따로 분리
"""

from nltk.tokenize import TreebankWordTokenizer
tokenizer = TreebankWordTokenizer()
print(tokenizer.tokenize(sentence))

"""# 한국어 토큰화가 어려운 이유
1. 한국어는 교착어이다.
2. 한국어는 띄어쓰기가 잘 지켜지지 않는다. 
3. 주어생략은 물론 어순도 중요하지 않다.
4. 한자어라는 특성상 하나의 음절도 각자 다른의미를 갖는다.
"""

# !pip install konlpy

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git
# %cd Mecab-ko-for-Google-Colab
!bash install_mecab-ko_on_colab190912.sh

from konlpy.tag import Hannanum, Kkma, Okt, Mecab, Komoran
hannanum = Hannanum()
kkma = Kkma() 
komoran = Komoran()
okt = Okt()
mecab = Mecab()

sentence = "그 누구도, 잡념도 초대는 못해 오직 나와 선물로 받은 촛대만이 내 인생도, 내 입생 섬유도 날 가지지 못해 내 속내를 외치듯 알몸을 까놓고"

"""- nouns: 명사추출
- morphs: 형태소 분리
- pos: 형태소 분리 및 설명
"""

# 트위터에서 개발한 한국어 형태소 분리기(okt)
print(okt.nouns(sentence))
print(okt.morphs(sentence))
print(okt.pos(sentence))

# 꼬꼬마
print(kkma.nouns(sentence))
print(kkma.morphs(sentence))
print(kkma.pos(sentence))

# komoran
print(komoran.nouns(sentence))
print(komoran.morphs(sentence))
print(komoran.pos(sentence))

# hannanum
print(hannanum.nouns(sentence))
print(hannanum.morphs(sentence))
print(hannanum.pos(sentence))
# 컨트롤 + D 똑같은 단어 선택 단축키

# mecab
print(mecab.nouns(sentence))
print(mecab.morphs(sentence))
print(mecab.pos(sentence))

"""## 정제(Cleaning)
- 정제 / 불필요한 데이터를 제거하는 작업
    - 한국어 분석에 집중하기 위해 숫자, 영어, 특수 기호들은 제거
    - 은, 는, 이, 가.... 제거
    - 띄어쓰기, 맞춤법을 확인해서 데이터를 더 깨끗하게 만드는 과정
---
- 정규표현식을 이용한 노이즈 데이터 제거
- 인코딩 문제 해결
- 등장 빈도가 적은 단어 제거 (단어에 대한 중요도 판단)
    - 등장 빈도가 2회 이하면 해당 단어를 제거한다.
- 길이가 짧은 단어 제거
    - 영어의 경우 i, by, at....
- 불용어 제거
    - 영어의 the -> 의미가 없다.
    - 한국어의 경우 그럼, 위하, 때, 있, 그것, 사실, 경우, 어떤, 은, 는, 을, 를... 등등

## 정규화(Normalization)
- 보통 사람이 직접 규칙을 정한다.
- lemmatization
    - am, are, were, was -> be
    - has, had -> have
- 10, 158, 122 0 -> num(숫자가 별로 중요하지 않은 경우)
- ㅋ, ㅋㅋㅋㅋ, ㅋㅋㅋㅋㅋ -> ㅋㅋ
- wow, hmmmmmm, hmmmm, woooow -> wow, hmm
- 대소문자 통합

### 영어 정규화 - Stemming
영문법의 규칙에 의한 단어를 단순화 시키는 과정

- pre----, ize----, aunce, ---al, s

포터 스테머의 규칙
* Serialize -> serial
* Allowance -> allow
* Medical -> medic
* This -> thi
"""

from nltk.stem import PorterStemmer
porterstemmer = PorterStemmer()

sentence = "All the colors and personalities You can't see right through what I truly am You're hurting me without noticing I'm so, so broke like someone just robbed me"

words = word_tokenize(sentence)
print(words)

print([porterstemmer.stem(w) for w in words])

"""### 영어 정규화 - Lemmatization
- 뿌리단어를 찾아서 바꿔준다 -> 단어의 개수를 줄일 수 있다.
"""

nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer
wordnetlemmatization = WordNetLemmatizer()

words = [
         'Policy',
         'doing',
         'conga',
         'are',
         'felt',
         'container',
         'Loves',
         'Hello',
         'World',
         'Runtime',
         'Tool',
         'Maker'
]

print([wordnetlemmatization.lemmatize(w) for w in words])

"""## 한국어 정규화 - Stemming, Normalization"""

text = "모든 색과 성격들 내가 누군지에 의해 정확히 딱 볼 수 없어요. 당신은 알지 못하게 너를 아프게 해요. 난 누가 저를 강도질 한 것 같이 아무것도 없어요"

print(okt.morphs(text))
print(okt.morphs(text, stem=True)) # stemming : 어근 찾기

text2 = "반갑습니다띵킹입니닼ㅋㅋㅋㅋ."
print(okt.morphs(text2))
print(okt.morphs(text2, norm=True))
print(okt.morphs(text2, stem=True))
print(okt.morphs(text2, norm=True, stem=True))

!pip install soynlp

from soynlp.normalizer import emoticon_normalize
print(emoticon_normalize("엌ㅋㅋㅋㅋㅋㅋㅋㅋㅋ큐ㅠㅠㅠㅠㅠㅠ" ,num_repeats=2))
print(emoticon_normalize("엌ㅋㅋㅋㅋㅋㅋㅋ큐ㅠㅠㅠㅠ" ,num_repeats=2))
print(emoticon_normalize("엌ㅋㅋㅋㅋㅋ큐ㅠㅠㅠ" ,num_repeats=2))
print(emoticon_normalize("엌ㅋㅋㅋㅋ큐ㅠㅠ" ,num_repeats=2))
print(emoticon_normalize("엌ㅋㅋㅋ큐ㅠ" ,num_repeats=2))

from soynlp.normalizer import repeat_normalize
print(repeat_normalize("엌ㅋㅋㅋㅋㅋㅋㅋㅋㅋ큐ㅠㅠㅠㅠㅠㅠ" ,num_repeats=2))
print(repeat_normalize("하차차차차차차찿" ,num_repeats=2))

!pip install git+https://github.com/ssut/py-hanspell.git

!pip install git+https://github.com/haven-jeon/PyKoSpacing.git

# KoSpacing 활용하기
from pykospacing import spacing
text = "2번은 산타클로스가 아니라 카고 컬트 같네요태평양전쟁때 미군이 태평양 섬들에 기지 세워놓았다가 사라져서 그런지 거기 살던 원주민들이 총모양에 미군복 모양의 옷이나 문신 하고 미군제식 비행기 뜰때 수신호 같은거 따라하던 카고컬트를 진짜 카고컬트를 재현해버린거같네요"
text = text.replace(" ", "")
spacing_text = spacing(text)
print(spacing_text)

from hanspell import spell_checker
sentence = "맛춤뻡 틀리면 외 않되?"
spelled_sent = spell_checker.check(sentence)

hanspell_sentence = spelled_sent.checked
print(hanspell_sentence)

kospacing_text = spacing(text) # kospacing 활용해서 띄어쓰기 교정
hanspell_text = spell_checker.check(text).checked

print(kospacing_text)
print(hanspell_text)

"""## 텍스트 정제
정규 표현식 사용하기
- 영어만 추출하는 정규식 표현 
    - [a-zA-z]
- 한글 추출 정규식
    - [ㄱ-ㅎㅏ-ㅣ가-힣]
    - [ㄱ-ㅎ가-힣]
    - [가-힣]
"""

import re
eng_sent = '\n\n\n\n\n\n\n hi 1 2 3 lll hell world FAQ \n\n\n\n\n\n\n'
eng_sent

print(eng_sent)

# 1. 특수기호 제거
eng_sent = re.sub("[^a-zA-Z]", " ", eng_sent)
print(eng_sent)

eng_sent = ' '.join([w for w in eng_sent.split() if len(w) > 2])

eng_sent

"""## Stopwords 설정하기
불용어 * 자주 등장하는데 실제 의미 분석을 하는데 있어서 별 필요 없는 단어들

### 영어 stopwords
"""

# import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
print(stopwords.words('english'))

example_tokens = "Family is not an important thing It's everything"
stop_words = set(stopwords.words('english'))

word_tokens = word_tokenize(example_tokens)
print(word_tokens)

result = []
for w in word_tokens:
    if w not in stop_words:
        result.append(w)

print("원문 : ", word_tokens)
print("불용 제거 후 : ", result)

"""## 한국어 stopwords"""

example = "와 이런것도 영화라고 차라리 뮤직비디오를 만들지 그래?"
word_tokens = okt.morphs(example)
print(word_tokens)

stop_words = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로' ,'자','에','와','한','하다', '것']
print(stop_words)

result = [word for word in word_tokens if not word in stop_words]
print("원문 : ", word_tokens)
print("불용 제거 후 : ", result)

