# -*- coding: utf-8 -*-
"""10_CNN_Advanced.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wruroFP-Ut7AIiXzX3-2U6tNSJS4gvqW

# Advanced한 내용
* DataGenerator를 이용한 데이터 관리
* @tf.session을 이용한 훈련
* 중급자 수준의 활용?
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import datasets
import numpy as np

import matplotlib.pyplot as plt
# %matplotlib inline

"""## DataGenerator
1. 사이킷 런의 train_test_split의 업그레이드 된 버전
2. 셔플의 기능이 추가, 배치의 기능 까지 추가
3. label의 관리가 필요가 없음.

* from tensor_slices() -> 데이터 세트 만들기
* shuffle() -> 섞기
* batch() -> 미니배치 만들기
"""

mnist = tf.keras.datasets.mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()

X_train = X_train[..., tf.newaxis]
X_test = X_test[..., tf.newaxis]

X_train, X_test = X_train / 255., X_test / 255.

"""## tf.data 사용하기"""

# 시작지점
# 데이터를 병렬로 불러올 수 있게 된다.
train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))

# 훈련데이터 generator 만들기
train_ds = train_ds.shuffle(1000) # 버퍼사이즈 지정 1000정도가 적당.
train_ds = train_ds.batch(32) # 미니배치 사이즈

# 테스트데이터 generator 만들기
# 테스트 데이터는 셔플링할 필요하 없다. 
test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))
test_ds = test_ds.batch(32) # 미니배치 사이즈

"""## 시각화
* take() -> 함수를 이용하면 배치 사이즈만큼 가져온다.
"""

# 현재 각 배치의 형상(32, 28, 28, 1)
for image_batch, label in train_ds.take(2):
    plt.title(str(label[0]))
    plt.imshow(image_batch[0, ..., 0], 'gray')
    plt.show()

"""## Keras를 이용해서 CNN레이어 만들기"""

# 레이어 쌓기

# 1. input형상 준비, 분류할 클래스 개수
# X_train의 형상이 들어간다. X_train[0].shape같은게 가능.
input_shape = (28, 28, 1)
num_classes = 10

# 2. input layer 만들기
inputs = layers.Input(shape=input_shape)

# 3. 필요한 레이어 쌓기
# Feature Extraction을 할 수 있는 CNN 레이어를 구성
net = layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='SAME')(inputs)
net = layers.Activation('relu')(net)
net = layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='SAME')(net)
net = layers.Activation('relu')(net)
net = layers.MaxPool2D((2, 2))(net)
# dropout 데이터가 너무 많고 피쳐가 많으면 dropout을 쓰는게 데이터의 다양성에 도움을 줌.
net = layers.Dropout(0.25)(net) 

net = layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='SAME')(net)
net = layers.Activation('relu')(net)
net = layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='SAME')(net)
net = layers.Activation('relu')(net)
net = layers.MaxPool2D((2, 2))(net)
net = layers.Dropout(0.25)(net) 

# Fully Connected(Dense, Affine)
net = layers.Flatten()(net) # 데이터를 쫙 펴준다.
net = layers.Dense(512)(net)
net = layers.Activation('relu')(net)
net = layers.Dropout(0.25)(net) 


# 출력층 설계
net = layers.Dense(num_classes)(net)
net = layers.Activation('softmax')(net)

# 최종 모델 만들기
model = tf.keras.Model(inputs=inputs, outputs=net, name='Basic_CNN')

# model.summary()

# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
# model.fit(train_ds, epochs=1)

"""# 텐서플로우를 중급자 수준으로 사용하기

## 최적화 함수 따로 빼기
- loss function
- Optimizer
"""

loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
optimizer = tf.keras.optimizers.Adam()

"""배치를 활용하고 있기 때문에, 각 배치마다의 loss, accuracy에 대한 평균을 구해준다.

즉 평가 방법을 각 배치마다의 평균을 낸다.
"""

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')

"""## 훈련 시키기, 훈련테스트
- @tf.function - 그래프만 미리 만들어 놓고, 실제 학습이 시작되면 실행 되는 함수 만들기
"""

@tf.function # tensorflow session에서 작동되는 함수 만들기, 즉 훈련을 할때만 실행된다.
def train_step(images, labels):
    # 자동 미분 (오차 역전파) 수행하는 객체 생성
    # 최초 설정 없으면 안됨.
    with tf.GradientTape() as tape:
        # 한번도 훈련하지 않은 데이터를 그냥 predictions에 넣었다.
        predictions = model(images) # 모델이 훈련 모드로 작동한다.
        loss = loss_object(labels, predictions) # 최초오차를 통해 오차 역전파과정.

    # 미분 적용하기
    """
    현재 가중치에 대한 loss 값
    이전 층에서 넘어온 입력값(훈련 가능한 변수 -> W, B)
    기울기를 구했음.
    """
    gradients = tape.gradient(loss, model.trainable_variables)

    """
    기울기를 모델에 적용시키는 단계
    최적화
    loss가 0에 가까워지게 해주는 편향과 가중치를 적용하게 해주는 친구들
    경사 하강법
    위에서 구한 기울기를 이용한 최적화 단계
    """
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))


    # 배치에 대한 평균 손실 구하기
    train_loss(loss)

    # 배치 평균 정확도 구하기
    train_accuracy(labels, predictions)

"""## 테스트 구현하기"""

@tf.function
def test_step(images, labels):
    predictions = model(images)
    t_loss = loss_object(labels, predictions)

    test_loss(t_loss)
    test_accuracy(labels, predictions)

epochs = 5
for epoch in range(epochs):

    # 전체 데이터 훈련
    for images, labels in train_ds:
        train_step(images, labels)

    # 이어서 테스트 하기
    for test_images, test_labels in test_ds:
        test_step(test_images, test_labels)

    template = "epoch {}, train_loss : {:.6f} / train_accuracy : {:.6f} --- test_loss : {:.6f} / test_accuracy : {:.6f}"
    print(template.format(epoch + 1, 
                          train_loss.result(), 
                          train_accuracy.result() * 100,
                          test_loss.result(), 
                          test_accuracy.result() * 100))

"""## 모델 평가와 히스토리 보기"""

# 레이어 쌓기

# 1. input형상 준비, 분류할 클래스 개수
# X_train의 형상이 들어간다. X_train[0].shape같은게 가능.
input_shape = (28, 28, 1)
num_classes = 10

# 2. input layer 만들기
inputs = layers.Input(shape=input_shape)

# 3. 필요한 레이어 쌓기
# Feature Extraction을 할 수 있는 CNN 레이어를 구성
net = layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='SAME')(inputs)
net = layers.Activation('relu')(net)
net = layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='SAME')(net)
net = layers.Activation('relu')(net)
net = layers.MaxPool2D((2, 2))(net)
# dropout 데이터가 너무 많고 피쳐가 많으면 dropout을 쓰는게 데이터의 다양성에 도움을 줌.
net = layers.Dropout(0.25)(net) 

net = layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='SAME')(net)
net = layers.Activation('relu')(net)
net = layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='SAME')(net)
net = layers.Activation('relu')(net)
net = layers.MaxPool2D((2, 2))(net)
net = layers.Dropout(0.25)(net) 

# Fully Connected(Dense, Affine)
net = layers.Flatten()(net) # 데이터를 쫙 펴준다.
net = layers.Dense(512)(net)
net = layers.Activation('relu')(net)
net = layers.Dropout(0.25)(net) 


# 출력층 설계
net = layers.Dense(num_classes)(net)
net = layers.Activation('softmax')(net)

# 최종 모델 만들기
model = tf.keras.Model(inputs=inputs, outputs=net, name='Basic_CNN')

"""## Evaluating
- 학습된 모델 확인
"""

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])
hist = model.fit(train_ds, epochs=10) # 훈련데이터에 대한 에폭 당 히스토리 확인가능

hist.history

# 테스트 데이터에 대한 확인
model.evaluate(test_ds)

