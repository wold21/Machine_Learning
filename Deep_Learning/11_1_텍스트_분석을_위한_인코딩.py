# -*- coding: utf-8 -*-
"""11_1_텍스트 분석을 위한 인코딩.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lKslKh4uLKBH65mgbD_8qUA_e4GFhD-r

# 텍스트의 수치화
* Integer Encoding
    - 단어 토큰화 이후에 각 단어에 고유한 정수를 부여
    - 중복이 허용되지 않는 모든 단어들의 집합을 만들기

* Padding
    - 길이를 맞춰주기 위해서 짧은 길이의 데이터에 padding 작업을 함.
    - 강제로 다른 길이의 문자들을 맞춰줌.
    - 모든문장에 대해서 정수 인코딩을 수행 했을때 문장마다의 길이가 다를 수 있다.
    - 가상의 단어를 만들어 준다.(보통 \<pad>으로 만들고 0 추가)

## 텍스트의 벡터화 vectorization
- 단어 리스트 / [오늘 점심 수제비 내일 짬뽕 저녁 소고기, OOV(n)]
1. One-Hot encoding
2. DTM(Document Term Matrix)
    - 하나의 문장에 어떤 단어가 몇번 나왔는지
    - 단어 집합에 없다면 집합의 맨 처음이나 끝에 OOV라는 토큰을 만들 수 있다. 
3. TF-IDF
    - 단어빈도(Term Frequency) - 역(Inverse) 문서 빈도(Document Frequency)
    - TF값과 IDF를 곱한 것
    - 문서의 유사도, 검색 시스템에서 결과의 순위를 구하는 일 등에 사용된다.
    - **인공 신경망의 입력으로 자주 사용된다.**
"""

import nltk
nltk.download('punkt')
text = "A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain."

from nltk.tokenize import sent_tokenize

text_token = sent_tokenize(text)
print(text_token)

nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

sentences = []
stop_words = set(stopwords.words('english')) # 불용어 불러오기 (불용어 사전)

for i in text_token:
    sentence = word_tokenize(i) # 단어 토큰화
    result = []

    for word in sentence:
        word = word.lower()

        if word not in stop_words: # 불용어를 제거
                if len(word) > 2: # 단어가 길이가 2이하인 경우 추가적으로 제거
                    result.append(word)
    sentences.append(result)
print(sentences)

# 가장 기초적인 방법으로 단어 집합 만들기
from collections import Counter

words = sum(sentences, [])
print(words)

"""단어 집합을 만들 때 여러 기준이 있다.
1. 단어의 빈도 수 많이 나올 수록 뒤쪽(앞쪽)에 있다. (많이 통용되는 방법)
2. 가나다순으로 저장
3. 그냥 들어가는대로 넣어도 상관은 없음(자주 사용되는 방법은 아니다.)
"""

vocab = Counter(words)
print(vocab)
# 딕셔너리 형태

vocab['barber']

"""## Integer Encoding
1. 빈도수가 높은 순대로 정렬
2. 높을수록 낮은 번호를 부여
3. 빈도수가 낮은 단어들을 제외시키면서 단어 집합의 크기를 조절하기 편하다.
"""

# 빈도수가 높은 순서대로 정렬하기
vocab_sorted = sorted(vocab.items(), key=lambda x : x[1], reverse=True)
print(vocab_sorted)

# 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스 부여하기
word2idx = {}
i=0

# 각 단어의 빈도수 불러오기
for (word, frequency) in vocab_sorted:

    # 단어의 빈도가 2개 이상인 것만 단어 집합에 추가하기
    if frequency > 1:
        i = i + 1
        word2idx[word] = i
print(word2idx)

"""만들어진 단어 집합에서 상위 5개의 단어만 사용하도록 하기"""

# vocab_size -> 딥러닝 모델링을 할 때 인풋데이터의 형상이 됨.
vocab_size = 5
words_frequency = [w for w, c in word2idx.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거

for w in words_frequency:
    del word2idx[w] # 위에서 선택된 단어 제거
print(word2idx)

"""### OOV추가 (out of vocabulary)
단어 사전에 없는 단어

단어사전 제일 뒤에 oov또는 UNK(Unknown)토큰을 추가해 줍니다.
"""

word2idx['UNK'] = 6
print(word2idx)

sentences

encoded = []
for s in sentences:
    temp = []

    for w in s:
        try:
            temp.append(word2idx[w])
        except:
            temp.append(word2idx['UNK'])

    encoded.append(temp)
encoded

"""## 단어 집합 생성 및 정수 인코딩을 tensorflow로 
- keras.preprocessing.text.Tokenizer를 이용하면 된다.
"""

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentences)
print(tokenizer.word_index)
print(tokenizer.word_counts)

print(tokenizer.texts_to_sequences(sentences))

vocab_size = 5
tokenizer = Tokenizer(num_words=vocab_size +1) # 상위 5개의 단어만 사용하기
# padding 때문에 +1을 해주는 것.
tokenizer.fit_on_texts(sentences)

tokenizer.texts_to_sequences(sentences)

vocab_size = 5
# 빈도수 상위 5개 단어만 사용하기. 숫자 0과 OOV를 고려해야하기 때문에 단어 집합의 크기는 vocab_size + 2가 되어야 한다.
tokenizer = Tokenizer(num_words=vocab_size + 2, oov_token='OOV')
tokenizer.fit_on_texts(sentences)

tokenizer.texts_to_sequences(sentences)

print("단어 oov의 인덱스 : {}".format(tokenizer.word_index['OOV']))

"""## 한국어 Tokenization & Integer Encoding까지 TensorFlow로"""

!pip install konlpy

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git
# %cd Mecab-ko-for-Google-Colab
!bash install_mecab-ko_on_colab190912.sh

import pandas as pd
import numpy as np
import urllib.request
from tensorflow.keras.preprocessing.text import Tokenizer

urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt", filename="ratings_test.txt")

train_data = pd.read_table('ratings_test.txt')
train_data.info()

train_data.head()

"""중복이 있는지 확인"""

train_data['document'].nunique()

# 중복데이터 제거 및 null제거
train_data.drop_duplicates(subset=['document'], inplace=True) # 중복제거
train_data = train_data.dropna(how='any') # Null값이 존재하는 행 제거

len(train_data)

train_data['document'] = train_data['document'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]", "")
train_data.head()

train_data['document'].replace('', np.nan, inplace=True)
print(train_data.isnull().sum())

# document가 nan값인 행 날리기
# DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)
train_data = train_data.dropna(how='any')
print(len(train_data))

"""불용어 정의"""

stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']

from konlpy.tag import Okt
okt = Okt()
okt.morphs("와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔했네 하.하.하.", stem=True)

"""형태소 분리를 이용한 토큰화 후 불용어 처리까지"""

X_train = []
for sentence in train_data['document']:
    temp_X = []
    temp_X = okt.morphs(sentence, stem=True) # 토큰화
    temp_X = [ word for word in temp_X if not word in stopwords] # 불용어 제거
    X_train.append(temp_X)

X_train[:10]

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

print(tokenizer.word_index)

print(tokenizer.word_counts)

print(tokenizer.word_counts['바람'])

"""인코딩 수행하기"""

encoded = tokenizer.texts_to_sequences(X_train)

X_train[:5]

encoded[:5]

tokenizer.word_index['굳다']

tokenizer.word_index['지루하다']

