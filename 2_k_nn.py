# -*- coding: utf-8 -*-
"""2_k-NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cKGhlFYMukM5FF7ZxhYLCV9qf1dfNenO
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import mglearn

"""#  k-NN 분류"""

# 테스트 데이터 세트 가져오기
# y결정 짓기 위한 X값
# 많은 데이터들에 대한 X값
# 그래서 대문자를 많이 씀.
# 소문자면 데이터 한개,  많으면 대문자
X,  y  =  mglearn.datasets.make_forge()

X_train,  X_test,  y_train,  y_test  =  train_test_split(X,  y,  random_state=42)

# k-NN  모델에는 이웃의 개수가 들어간다.(이웃의 개수가 하이퍼 파라미터로써, 복잡도를 조절한다.)
clf = KNeighborsClassifier(n_neighbors=3)
clf

# metric='minkowski' 거리의 이름인데 러시아 수학자의 이름이고 유클라디안 방식이다.
# n_jobs=None  사용할  CPU의 개수

# 모델 훈련   -  fit 메소드 활용, 지도학습 알고리즘 훈련시에는 X_train, y_train을 넣는다.(적용)
clf.fit(X_train, y_train)

# 예측은 predict  함수를 활용,  예측할 데이터를 넣어주면 됩니다.
clf.predict(X_test)

# 훈련용 데이터와 테스트 데이터의 형상(shape)은 언제나 일치해야한다.
print("훈련 데이터의 Shape : {}".format(X_train.shape))
print("테스트 데이터의 Shape : {}".format(X_test.shape))

# 앞쪽은 행  뒤쪽은 특성
# 특성을 맞춰주어야함.

score = clf.score(X_test, y_test)
print(score)

# 얼마나 많이 맞췄냐 100개중에 86개 정도.

"""#  k-NN  회귀
<br>
오차율이 나오기 시작

R는 결정대수

y는 target

y^은  predict(예측값)

y-는 target mean(평균)(y들의 평균)
"""

#  사이키런의 특징 상 알고리즘명 뒤에 Classifier, Regressor가 붙는다..
from sklearn.neighbors import KNeighborsRegressor

X, y = mglearn.datasets.make_wave(n_samples=40)

X_train,  X_test,  y_train,  y_test  =  train_test_split(X,  y,  random_state=0)

reg_1 = KNeighborsRegressor(n_neighbors = 1)
reg_3 = KNeighborsRegressor(n_neighbors = 3)

reg_1.fit(X_train, y_train)
reg_3.fit(X_train, y_train)

# 이웃의 개수가 1일 때와 3일 때의 결정  계수(점수)  확인하기
# 회귀에서의 score 함수는 결정 계수를 보여준다.

print("이웃의 개수가 1개 일때  :  {:.2f}".format(reg_1.score(X_test, y_test)))
print("이웃의 개수가 3개 일때  :  {:.2f}".format(reg_3.score(X_test, y_test)))

print("이웃의 개수가 1개 일때  :  {:.2f}".format(reg_1.score(X_train, y_train)))
print("이웃의 개수가 3개 일때  :  {:.2f}".format(reg_3.score(X_train, y_train)))