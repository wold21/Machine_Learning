# -*- coding: utf-8 -*-
"""2_linear regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17EwyLPMhHtJ6aTXzn-8ApcGLqysu7MIl
"""

from sklearn.linear_model  import  LinearRegression
from sklearn.model_selection import train_test_split
import mglearn

#  linear regression
#  오차를 제일 줄일 수 있는 (  데이터를 가장 잘 표현하는 )  직선을 긋기 위해 기울기와 절편을 구하는 알고리즘

X, y = mglearn.datasets.make_wave(n_samples=60)

X_train,  X_test,  y_train, y_test  =  train_test_split(X,  y,  random_state=42)

lin_reg = LinearRegression().fit(X_train, y_train) # 훈련

print("기울기(w값)  :  {}".format(lin_reg.coef_))
print("기울기(b값)  :  {}".format(lin_reg.intercept_))

print("훈련 세트 점수 :  {:.2f}".format(lin_reg.score(X_train, y_train)))
print("테스트 세트 점수 :  {:.2f}".format(lin_reg.score(X_test, y_test)))

# 선형회귀의 단점은 데이터의 features수가 모자르면 과소적합이 된다.

"""#  릿지(RIdge) 회귀

릿지도 회귀를 위한 선형 모델이므로 최소 제곱법 같은 예측 함수를 사용한다.  MSE

k-NN은 이웃의 갯수에 선형회귀는 가중치(w)에 따라 예측에 영향을 준다.

그 가중치를 바꿀 방법이 없지만 릿지(MSE)와 라쏘(MAE)에는 그것을 보완할 패널티라는 것이 존재함.

y = w1x1 + w2x2 + b

가중치를 0에 '가깝게' 만듬
"""

from sklearn.linear_model import Ridge

# 보스턴 주택가격 데이터셋 가져오기
X, y = mglearn.datasets.load_extended_boston()

X_train,  X_test,  y_train, y_test  =  train_test_split(X,  y,  random_state=0)


lin_reg = LinearRegression().fit(X_train, y_train)
ridge1 = Ridge().fit(X_train, y_train)

# 일반 linearRegression과 비교
print("선형 회귀 모델 :  {:.2f}  /  {:.2f}".format(lin_reg.score(X_train, y_train), 
                                                                        lin_reg.score(X_test, y_test)))

print("릿지 회귀 모델 :  {:.2f}  /  {:.2f}".format(ridge1.score(X_train, y_train), 
                                                                        ridge1.score(X_test, y_test)))

# 값을 보고 모델이 과소, 과대를 가지는지.
# 모델을 파악
# 알고리즘상 선형보다 릿지가 더 결과는 좋았다.

# 소수점으로 하면 가중치를 증가
# 10의 단위로 하면 가중치를 줄임
alpha = 10

ridge10 = Ridge(alpha=alpha).fit(X_train, y_train)

print("Ridge10 : {:.2f} / {:.2f}".format(ridge10.score(X_train, y_train),
                                                      ridge10.score(X_test, y_test)))

# 소수점으로 하면 가중치를 증가
# 트레인과 테스트의 격차가 더 벌어짐
# 과대 성향을 가진다.
alpha = 0.1

ridge01 = Ridge(alpha=alpha).fit(X_train, y_train)

print("Ridge10 : {:.2f} / {:.2f}".format(ridge01.score(X_train, y_train),
                                                      ridge01.score(X_test, y_test)))

"""# 라쏘(Lasso)회귀 이상치에 좋다.

이 특성이 필요 없을 거 같으면 가중치를 아예 0으로 만들어서 선택조차 안되게 만들 수 있다.

일부 특성만 사용할 수 있다는 말이되기도 함.

이것을 특성 자동 선택이라고 부른다.
"""

from sklearn.linear_model import Lasso
import numpy as np

lasso = Lasso().fit(X_train, y_train)
print("훈련 세트 점수 : {:.2f}".format(lasso.score(X_train, y_train)))
print("테스트 세트 점수 : {:.2f}".format(lasso.score(X_test, y_test)))
print("사용한 특성의 수 : {}".format(np.sum(lasso.coef_ != 0)))

lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)
print("훈련 세트 점수 : {:.2f}".format(lasso001.score(X_train, y_train)))
print("테스트 세트 점수 : {:.2f}".format(lasso001.score(X_test, y_test)))
print("사용한 특성의 수 : {}".format(np.sum(lasso001.coef_ != 0)))

lasso0001 = Lasso(alpha=0.0001, max_iter=1000000).fit(X_train, y_train)

print("훈련 세트 점수 : {:.2f}".format(lasso0001.score(X_train, y_train)))
print("테스트 세트 점수 : {:.2f}".format(lasso0001.score(X_test, y_test)))
print("사용한 특성의 수 : {}".format(np.sum(lasso0001.coef_ != 0)))

